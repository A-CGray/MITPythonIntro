1
00:00:00,000 --> 00:00:00,050

2
00:00:00,050 --> 00:00:02,490
The following content is
provided under a Creative

3
00:00:02,490 --> 00:00:03,900
Commons license.

4
00:00:03,900 --> 00:00:06,940
Your support will help MIT
OpenCourseWare continue to

5
00:00:06,940 --> 00:00:10,600
offer high quality educational
resources for free.

6
00:00:10,600 --> 00:00:13,490
To make a donation or view
additional materials from

7
00:00:13,490 --> 00:00:19,320
hundreds of MIT courses, visit
MIT OpenCourseWare at

8
00:00:19,320 --> 00:00:21,882
ocw.mit.edu.

9
00:00:21,882 --> 00:00:26,250
PROFESSOR: Today, we're moving
on to what will be a major

10
00:00:26,250 --> 00:00:31,320
unit of the course, which is
the topic of efficiency.

11
00:00:31,320 --> 00:00:35,930
Thus far, we focused our
attention on the admittedly

12
00:00:35,930 --> 00:00:39,610
more important problem, getting
our programs to work,

13
00:00:39,610 --> 00:00:42,110
i.e., to do what we
want them to do.

14
00:00:42,110 --> 00:00:46,510
For the next several lectures, I
want to talk about how do we

15
00:00:46,510 --> 00:00:51,160
get them to work quickly
enough to be useful.

16
00:00:51,160 --> 00:00:56,750
It is in practice often a very
important consideration in

17
00:00:56,750 --> 00:00:58,980
designing programs.

18
00:00:58,980 --> 00:01:04,300
The goal is not to make you
an expert in this topic.

19
00:01:04,300 --> 00:01:06,250
It's hard to be an expert
in this topic.

20
00:01:06,250 --> 00:01:08,550
I'm certainly not an expert.

21
00:01:08,550 --> 00:01:12,450
But I want to give you some
intuition about how to

22
00:01:12,450 --> 00:01:16,730
approach the question of
efficiency, how to understand

23
00:01:16,730 --> 00:01:22,990
why some programs take much
longer to run than others, and

24
00:01:22,990 --> 00:01:26,610
how to go about writing programs
that will finish

25
00:01:26,610 --> 00:01:29,160
before you die.

26
00:01:29,160 --> 00:01:31,400
And we'll see that if you
write things wrong, the

27
00:01:31,400 --> 00:01:36,830
programs could, in principle,
run longer than you can.

28
00:01:36,830 --> 00:01:40,100
So why is efficiency
so important?

29
00:01:40,100 --> 00:01:43,710
Earlier in the term, I started
to spend some time talking

30
00:01:43,710 --> 00:01:47,530
about how really fast computers
are and showing you

31
00:01:47,530 --> 00:01:51,160
that we can use brute force
algorithms to solve fairly

32
00:01:51,160 --> 00:01:52,410
large problems.

33
00:01:52,410 --> 00:01:55,330

34
00:01:55,330 --> 00:01:58,820
The difficulty is that some of
the computational problems

35
00:01:58,820 --> 00:02:04,270
we're confronted with are not
fairly large but enormous.

36
00:02:04,270 --> 00:02:07,860
So for example, in my research
group where we work at the

37
00:02:07,860 --> 00:02:11,100
intersection of computer science
and medicine, we have

38
00:02:11,100 --> 00:02:16,810
a big database of roughly a
billion and half heart beats.

39
00:02:16,810 --> 00:02:19,990
And we routinely run
computations that run for two

40
00:02:19,990 --> 00:02:22,370
weeks on that data.

41
00:02:22,370 --> 00:02:25,130
And the only reason they
complete it in two weeks and

42
00:02:25,130 --> 00:02:28,620
not two years is we were
really careful about

43
00:02:28,620 --> 00:02:30,180
efficiency.

44
00:02:30,180 --> 00:02:32,390
So it really can matter.

45
00:02:32,390 --> 00:02:35,360
And increasingly, it matters
is we see the scale of

46
00:02:35,360 --> 00:02:39,290
problems growing.

47
00:02:39,290 --> 00:02:43,456
The thing I want you to take
home to remember is that

48
00:02:43,456 --> 00:02:48,760
efficiency is rarely about
clever coding.

49
00:02:48,760 --> 00:02:51,860
It's not about some little
trick that saves one

50
00:02:51,860 --> 00:02:55,470
instruction here or two
instructions there.

51
00:02:55,470 --> 00:02:58,775
It's really about choosing
the right algorithm.

52
00:02:58,775 --> 00:03:03,930

53
00:03:03,930 --> 00:03:13,140
So the take home message is
that efficiency is about

54
00:03:13,140 --> 00:03:18,975
algorithms, not about
coding details.

55
00:03:18,975 --> 00:03:22,440

56
00:03:22,440 --> 00:03:25,890
Clever algorithms are
hard to invent.

57
00:03:25,890 --> 00:03:29,150
A successful computer scientist
might invent maybe

58
00:03:29,150 --> 00:03:33,220
one in his or her
whole career.

59
00:03:33,220 --> 00:03:37,380
I have to say I invented zero
important algorithms in my

60
00:03:37,380 --> 00:03:39,630
whole career.

61
00:03:39,630 --> 00:03:43,710
Therefore, we don't depend upon
being able to do that.

62
00:03:43,710 --> 00:03:48,760
Instead what we depend upon
is problem reducing.

63
00:03:48,760 --> 00:03:54,550
When confronted with a problem,
we want to reduce it

64
00:03:54,550 --> 00:03:56,560
to a previously solved
problem.

65
00:03:56,560 --> 00:04:04,230

66
00:04:04,230 --> 00:04:11,510
And this is really often the key
to taking some problem and

67
00:04:11,510 --> 00:04:14,350
fitting into a useful
computation.

68
00:04:14,350 --> 00:04:17,370
We sit back, say, well, this
looks a little bit like this

69
00:04:17,370 --> 00:04:18,370
other problem.

70
00:04:18,370 --> 00:04:22,160
How come I transform my problem
to match a problem

71
00:04:22,160 --> 00:04:26,660
that some clever person already
knows how to solve?

72
00:04:26,660 --> 00:04:31,040
Before I spend time on problem
reduction, however, I want to

73
00:04:31,040 --> 00:04:34,500
draw back and look at the
general question of how do we

74
00:04:34,500 --> 00:04:36,830
think about efficiency.

75
00:04:36,830 --> 00:04:39,340
When we think about it, we
think about it in two

76
00:04:39,340 --> 00:04:46,750
dimensions, space and time.

77
00:04:46,750 --> 00:04:50,950
And as we'll see later in the
term, we can often trade one

78
00:04:50,950 --> 00:04:52,420
for the other.

79
00:04:52,420 --> 00:04:57,070
We can make a program run faster
by using more memory or

80
00:04:57,070 --> 00:05:01,420
use less memory at the cost of
making it run more slowly.

81
00:05:01,420 --> 00:05:04,550
For now, and the next
few lectures, I'm

82
00:05:04,550 --> 00:05:07,180
going to focus on time.

83
00:05:07,180 --> 00:05:10,280
Because really, that's mostly
what people worry about these

84
00:05:10,280 --> 00:05:11,600
days when they're dealing
with complexity.

85
00:05:11,600 --> 00:05:14,720

86
00:05:14,720 --> 00:05:18,540
So now, suppose I ask you the
question, how long does some

87
00:05:18,540 --> 00:05:23,000
algorithm implemented by
a program take to run?

88
00:05:23,000 --> 00:05:26,370
How would you go about answering
that question?

89
00:05:26,370 --> 00:05:28,560
Well, you could say, all right,
I'm going to run it on

90
00:05:28,560 --> 00:05:32,600
some computer on some
input and time it.

91
00:05:32,600 --> 00:05:33,270
Look at my watch.

92
00:05:33,270 --> 00:05:35,600
That took three minutes.

93
00:05:35,600 --> 00:05:38,710
I ran this other algorithm,
and it took two minutes.

94
00:05:38,710 --> 00:05:41,280
It's a better algorithm.

95
00:05:41,280 --> 00:05:47,360
Well, that would be really
a bad way to look at it.

96
00:05:47,360 --> 00:05:50,800
The reasons we don't think
about computational

97
00:05:50,800 --> 00:06:01,030
complexity, and that's really
what people call this topic in

98
00:06:01,030 --> 00:06:04,240
terms of how long a program
takes to run on a particular

99
00:06:04,240 --> 00:06:09,800
computer, and it's not
a stable measure.

100
00:06:09,800 --> 00:06:13,875
To do that, it's influenced by
the speed of the machine.

101
00:06:13,875 --> 00:06:18,950

102
00:06:18,950 --> 00:06:21,670
So a program that took 1 minute
on my computer might

103
00:06:21,670 --> 00:06:23,370
take 30 seconds on yours.

104
00:06:23,370 --> 00:06:28,150

105
00:06:28,150 --> 00:06:32,500
It has to do with the cleverness
of the Python

106
00:06:32,500 --> 00:06:33,750
implementation.

107
00:06:33,750 --> 00:06:37,030

108
00:06:37,030 --> 00:06:40,240
Maybe I have a better
implementation of Python than

109
00:06:40,240 --> 00:06:45,840
you do, so my programs will
run a little bit faster.

110
00:06:45,840 --> 00:06:49,590
But most importantly, the reason
we don't depend upon

111
00:06:49,590 --> 00:06:54,480
running programs is it depends
upon the input.

112
00:06:54,480 --> 00:07:02,560

113
00:07:02,560 --> 00:07:05,330
So I might choose one input for
which the program took 2

114
00:07:05,330 --> 00:07:08,675
minutes and another seemingly
similar input in

115
00:07:08,675 --> 00:07:11,620
which it took 1 hour.

116
00:07:11,620 --> 00:07:17,230
So I need to get some way to
talk about it more abstractly.

117
00:07:17,230 --> 00:07:21,440
The way we do that is
by counting the

118
00:07:21,440 --> 00:07:22,895
number of basic steps.

119
00:07:22,895 --> 00:07:36,130

120
00:07:36,130 --> 00:07:43,050
So we define some function,
say time, which maps the

121
00:07:43,050 --> 00:07:45,235
natural numbers to the
natural numbers.

122
00:07:45,235 --> 00:07:50,090

123
00:07:50,090 --> 00:07:54,020
The first n, in this case, the
first natural number, the

124
00:07:54,020 --> 00:08:07,130
argument corresponds to the size
of the input, how big an

125
00:08:07,130 --> 00:08:09,840
input do we want to run
the program on.

126
00:08:09,840 --> 00:08:12,340

127
00:08:12,340 --> 00:08:20,400
And the result of the function
is the number of steps that

128
00:08:20,400 --> 00:08:25,060
the computation will take for
an input of that size.

129
00:08:25,060 --> 00:08:28,460
I'll come back to this in a
little bit more precise detail

130
00:08:28,460 --> 00:08:30,460
momentarily.

131
00:08:30,460 --> 00:08:36,950
A step is an operation that
takes constant time.

132
00:08:36,950 --> 00:08:39,770
And that's important.

133
00:08:39,770 --> 00:08:42,460
So steps are not variable,
but they're constant.

134
00:08:42,460 --> 00:08:57,600

135
00:08:57,600 --> 00:09:01,460
So we have lots of these, for
example, an assignment, a

136
00:09:01,460 --> 00:09:05,550
comparison, an array
access, et cetera.

137
00:09:05,550 --> 00:09:11,600
In looking at computational
complexity in this course,

138
00:09:11,600 --> 00:09:14,650
we're going to use a model
of the computer.

139
00:09:14,650 --> 00:09:26,930
It's known as random access,
a random access machine,

140
00:09:26,930 --> 00:09:28,510
frequently abbreviated as RAM.

141
00:09:28,510 --> 00:09:32,390

142
00:09:32,390 --> 00:09:36,230
In a random access machine,
instructions are executed one

143
00:09:36,230 --> 00:09:41,570
after another, that is to
say they're sequential.

144
00:09:41,570 --> 00:09:43,315
Only one thing happens
at a time.

145
00:09:43,315 --> 00:09:46,300

146
00:09:46,300 --> 00:09:52,840
And we assume constant time
required to access memory.

147
00:09:52,840 --> 00:09:56,870

148
00:09:56,870 --> 00:10:02,150
So we can access at random any
object in memory in the same

149
00:10:02,150 --> 00:10:03,950
amount of time as any
other object.

150
00:10:03,950 --> 00:10:15,370

151
00:10:15,370 --> 00:10:18,180
In the early days of computers,
this model was not

152
00:10:18,180 --> 00:10:23,480
accurate, because memory
was often say, a tape.

153
00:10:23,480 --> 00:10:25,400
And if you wanted to read
something at the end of the

154
00:10:25,400 --> 00:10:29,490
tape, it took a lot longer to
read than something at the

155
00:10:29,490 --> 00:10:31,380
beginning of the tape.

156
00:10:31,380 --> 00:10:36,140
In modern computers, it's
also not quite accurate.

157
00:10:36,140 --> 00:10:40,150
Modern computers have what's
called a memory hierarchy

158
00:10:40,150 --> 00:10:43,060
where you have levels of memory,
the level one cache,

159
00:10:43,060 --> 00:10:46,090
the level two cache,
the actual memory.

160
00:10:46,090 --> 00:10:50,280
And it can differ by say a
factor of a 100, how long it

161
00:10:50,280 --> 00:10:53,160
takes access data
depending upon

162
00:10:53,160 --> 00:10:55,560
whether it's in the cache.

163
00:10:55,560 --> 00:11:01,140
The cache keeps track of
recently accessed objects.

164
00:11:01,140 --> 00:11:04,360
Nevertheless, if we start
going into that level of

165
00:11:04,360 --> 00:11:08,870
detail, we end up losing the
forest for the trees.

166
00:11:08,870 --> 00:11:12,060
So almost everybody when they
actually try and analyze

167
00:11:12,060 --> 00:11:18,060
algorithms typically works
with this model.

168
00:11:18,060 --> 00:11:20,620
We also know in modern computers
that some things

169
00:11:20,620 --> 00:11:23,480
happen in parallel.

170
00:11:23,480 --> 00:11:25,600
But again, for most of
us, these will be

171
00:11:25,600 --> 00:11:27,620
second order effects.

172
00:11:27,620 --> 00:11:31,910
And the random access model is
quite good for understanding

173
00:11:31,910 --> 00:11:34,640
algorithms.

174
00:11:34,640 --> 00:11:39,640
Now when we think about how long
an algorithm will take to

175
00:11:39,640 --> 00:11:43,420
run, there are several
different ways we

176
00:11:43,420 --> 00:11:45,570
could look at it.

177
00:11:45,570 --> 00:11:47,415
We could think of
the best case.

178
00:11:47,415 --> 00:11:52,180

179
00:11:52,180 --> 00:11:56,170
And as we think about these
things, as a concrete example,

180
00:11:56,170 --> 00:11:58,780
we can think about
linear search.

181
00:11:58,780 --> 00:12:01,500
So let's say we have
an algorithm that's

182
00:12:01,500 --> 00:12:03,470
using linear search.

183
00:12:03,470 --> 00:12:06,730
We've looked at that before to
find out whether or not an

184
00:12:06,730 --> 00:12:09,050
element is in the list.

185
00:12:09,050 --> 00:12:12,510
Well, the best case would be
that the first element is

186
00:12:12,510 --> 00:12:15,990
three, and I'm searching for 3,
and I find it right away,

187
00:12:15,990 --> 00:12:18,560
and I stop.

188
00:12:18,560 --> 00:12:25,460
So that would be my best
case complexity.

189
00:12:25,460 --> 00:12:30,390
It's the minimum running time
over all possible inputs.

190
00:12:30,390 --> 00:12:31,640
Is the best case.

191
00:12:31,640 --> 00:12:33,720

192
00:12:33,720 --> 00:12:35,595
I can also look at
the worst case.

193
00:12:35,595 --> 00:12:39,050

194
00:12:39,050 --> 00:12:40,650
What's the worst case
for linear search?

195
00:12:40,650 --> 00:12:44,390

196
00:12:44,390 --> 00:12:45,210
It's not there.

197
00:12:45,210 --> 00:12:46,390
Exactly.

198
00:12:46,390 --> 00:12:50,230
So I go and I have to look at
every element, and whoops,

199
00:12:50,230 --> 00:12:52,690
it's not there.

200
00:12:52,690 --> 00:12:56,720
So the worst case is the maximum
over all possible

201
00:12:56,720 --> 00:12:58,235
inputs of a given size.

202
00:12:58,235 --> 00:13:01,180

203
00:13:01,180 --> 00:13:05,770
The size here is the
length of the list.

204
00:13:05,770 --> 00:13:12,980
And then I can ask what's the
expected or average case, what

205
00:13:12,980 --> 00:13:14,450
would happen most of the time.

206
00:13:14,450 --> 00:13:20,220

207
00:13:20,220 --> 00:13:24,140
The expected case seems, in
principle, like the one we

208
00:13:24,140 --> 00:13:26,710
should care about.

209
00:13:26,710 --> 00:13:31,180
But the truth is when we do
algorithmic analysis, we

210
00:13:31,180 --> 00:13:34,500
almost never deal with
the expected case

211
00:13:34,500 --> 00:13:35,750
because it's too hard.

212
00:13:35,750 --> 00:13:38,590

213
00:13:38,590 --> 00:13:43,430
We think about the expected case
for say linear search, we

214
00:13:43,430 --> 00:13:49,670
can't talk about it without some
detailed model of what

215
00:13:49,670 --> 00:13:52,660
the list itself looks like, what
elements are in it, and

216
00:13:52,660 --> 00:13:56,180
what the distribution of
queries looks like.

217
00:13:56,180 --> 00:13:58,930
Are we most of the time asking
for elements that are not in

218
00:13:58,930 --> 00:14:02,320
the list in which case
the expected

219
00:14:02,320 --> 00:14:03,635
value is out here somewhere?

220
00:14:03,635 --> 00:14:06,510

221
00:14:06,510 --> 00:14:08,940
Or are we most of the time
looking for things that are in

222
00:14:08,940 --> 00:14:11,980
the list in which case the
expected value would be

223
00:14:11,980 --> 00:14:16,600
somewhere near halfway through
the length of the list?

224
00:14:16,600 --> 00:14:17,850
We don't know those things.

225
00:14:17,850 --> 00:14:22,110
We have a tough time modeling
expected value.

226
00:14:22,110 --> 00:14:26,000
And one of the things we know is
that frequently we don't --

227
00:14:26,000 --> 00:14:27,600
when we release a program --

228
00:14:27,600 --> 00:14:31,360
have a good sense of how people
will actually use it.

229
00:14:31,360 --> 00:14:35,360
And so we don't usually
focus on that.

230
00:14:35,360 --> 00:14:38,420
Similarly, we don't usually
focus on the best case.

231
00:14:38,420 --> 00:14:41,430

232
00:14:41,430 --> 00:14:42,980
It would be nice.

233
00:14:42,980 --> 00:14:46,540
But you could imagine that it's
not really what we care

234
00:14:46,540 --> 00:14:51,120
about, what happens when
we get really lucky.

235
00:14:51,120 --> 00:14:52,920
Because we all believe
in Murphy's law.

236
00:14:52,920 --> 00:14:56,300

237
00:14:56,300 --> 00:15:00,010
If something bad can
happen, it will.

238
00:15:00,010 --> 00:15:05,820
And that's why complexity
analysis almost always focuses

239
00:15:05,820 --> 00:15:07,070
on the worst case.

240
00:15:07,070 --> 00:15:09,550

241
00:15:09,550 --> 00:15:16,405
What the worst case does is it
provides an upper bound.

242
00:15:16,405 --> 00:15:23,060

243
00:15:23,060 --> 00:15:26,330
How bad can things
possibly get?

244
00:15:26,330 --> 00:15:28,260
What's the worst that
can happen?

245
00:15:28,260 --> 00:15:31,160
And that's nice because
it means that

246
00:15:31,160 --> 00:15:33,280
there are no surprises.

247
00:15:33,280 --> 00:15:37,560
You say the worst that this
thing can do is look at every

248
00:15:37,560 --> 00:15:40,020
element of the list once.

249
00:15:40,020 --> 00:15:43,020
And so if I know that the list
is a million elements, I know,

250
00:15:43,020 --> 00:15:45,680
OK, it might have to do
a million comparisons.

251
00:15:45,680 --> 00:15:48,640
But it won't have to do any
more than a million.

252
00:15:48,640 --> 00:15:51,140
And so I won't be suddenly
surprised that it takes

253
00:15:51,140 --> 00:15:52,390
overnight to run.

254
00:15:52,390 --> 00:15:55,510

255
00:15:55,510 --> 00:15:58,325
Alas, the worst case
happens often.

256
00:15:58,325 --> 00:16:05,790

257
00:16:05,790 --> 00:16:09,130
We do frequently end up asking
whether something is in a

258
00:16:09,130 --> 00:16:10,380
list, and it's not.

259
00:16:10,380 --> 00:16:13,820

260
00:16:13,820 --> 00:16:18,260
So even though it seems
pessimistic to worry about the

261
00:16:18,260 --> 00:16:20,630
worst case, it is the right
one to worry about.

262
00:16:20,630 --> 00:16:23,380

263
00:16:23,380 --> 00:16:23,692
All right.

264
00:16:23,692 --> 00:16:26,590
Let's look at an example.

265
00:16:26,590 --> 00:16:29,900
So I've got a little
function here, f.

266
00:16:29,900 --> 00:16:30,825
You can see it here.

267
00:16:30,825 --> 00:16:32,145
It's on the handout as well.

268
00:16:32,145 --> 00:16:35,330

269
00:16:35,330 --> 00:16:37,680
First of all, what mathematical
function is f

270
00:16:37,680 --> 00:16:44,570
computing, just to force you
to look at it for a minute?

271
00:16:44,570 --> 00:16:45,480
What's it computing?

272
00:16:45,480 --> 00:16:47,560
Somebody?

273
00:16:47,560 --> 00:16:50,650
It is a function that should
be familiar to

274
00:16:50,650 --> 00:16:51,900
almost all of you.

275
00:16:51,900 --> 00:16:57,860

276
00:16:57,860 --> 00:16:59,850
Nobody?

277
00:16:59,850 --> 00:17:00,230
Pardon.

278
00:17:00,230 --> 00:17:01,530
AUDIENCE: Exponentiation.

279
00:17:01,530 --> 00:17:02,780
PROFESSOR: Exponentiation?

280
00:17:02,780 --> 00:17:04,890

281
00:17:04,890 --> 00:17:06,140
Don't think so.

282
00:17:06,140 --> 00:17:08,119

283
00:17:08,119 --> 00:17:09,440
But I appreciate
you're trying.

284
00:17:09,440 --> 00:17:13,040
It's worth some candy,
not a lot of candy,

285
00:17:13,040 --> 00:17:14,290
but a little candy.

286
00:17:14,290 --> 00:17:16,480

287
00:17:16,480 --> 00:17:16,924
Yeah?

288
00:17:16,924 --> 00:17:17,629
AUDIENCE: Factorial.

289
00:17:17,629 --> 00:17:18,790
PROFESSOR: Factorial.

290
00:17:18,790 --> 00:17:20,240
Exactly.

291
00:17:20,240 --> 00:17:21,490
It's computing factorial.

292
00:17:21,490 --> 00:17:24,790

293
00:17:24,790 --> 00:17:26,040
Great grab.

294
00:17:26,040 --> 00:17:29,340

295
00:17:29,340 --> 00:17:35,480
So let's think about how long
this will take to run in terms

296
00:17:35,480 --> 00:17:37,680
of the number of steps.

297
00:17:37,680 --> 00:17:48,850
Well, the first thing it does
is it executes an assertion.

298
00:17:48,850 --> 00:17:51,630
And for the sake of argument for
the moment, we can assume

299
00:17:51,630 --> 00:17:56,420
that most instructions in Python
will take one step.

300
00:17:56,420 --> 00:18:00,710
Then, it does an assignment,
so that's two steps.

301
00:18:00,710 --> 00:18:03,110
Then, it goes through
the loop.

302
00:18:03,110 --> 00:18:08,890
Each time through the loop, it
executes three steps, the test

303
00:18:08,890 --> 00:18:10,800
at the start of the
loop and the two

304
00:18:10,800 --> 00:18:13,420
instructions inside the loop.

305
00:18:13,420 --> 00:18:15,110
How many times does it
go through the loop?

306
00:18:15,110 --> 00:18:17,680

307
00:18:17,680 --> 00:18:20,380
Somebody?

308
00:18:20,380 --> 00:18:22,450
Right. n times.

309
00:18:22,450 --> 00:18:28,030
So it will be 2 plus
3 times n.

310
00:18:28,030 --> 00:18:30,590
And then it executes a return
statement at the end.

311
00:18:30,590 --> 00:18:33,510

312
00:18:33,510 --> 00:18:37,280
So if I want to write down the
function that characterizes

313
00:18:37,280 --> 00:18:42,250
the algorithm implemented by
this code, I say it's 2 plus 3

314
00:18:42,250 --> 00:18:45,440
times n plus 1.

315
00:18:45,440 --> 00:18:50,500
Well, I could do that.

316
00:18:50,500 --> 00:18:52,770
But it would be kind of silly.

317
00:18:52,770 --> 00:18:57,160
Let's say n equals 3,000.

318
00:18:57,160 --> 00:19:07,310
Well, if n equals 3,000, this
tells me that it takes 9,000--

319
00:19:07,310 --> 00:19:08,470
well, what does it take?

320
00:19:08,470 --> 00:19:11,690
9,003 steps.

321
00:19:11,690 --> 00:19:12,810
Right.

322
00:19:12,810 --> 00:19:18,430
Well, do I care whether
it's 9,000 or 9,003?

323
00:19:18,430 --> 00:19:20,280
I don't really.

324
00:19:20,280 --> 00:19:25,260
So in fact, when I look at
complexity, I tend to--

325
00:19:25,260 --> 00:19:29,015
I don't tend to I do ignore
additive constants.

326
00:19:29,015 --> 00:19:31,810

327
00:19:31,810 --> 00:19:36,270
So the fact that there's a 2
here and a 1 here doesn't

328
00:19:36,270 --> 00:19:38,490
really matter.

329
00:19:38,490 --> 00:19:41,600
So I say, well, if we're trying
to characterize this

330
00:19:41,600 --> 00:19:44,910
algorithm, let's ignore those.

331
00:19:44,910 --> 00:19:52,370
Because what I really
care about is growth

332
00:19:52,370 --> 00:19:53,840
with respect to size.

333
00:19:53,840 --> 00:19:57,170
How does the running time
grow as the size

334
00:19:57,170 --> 00:19:58,420
of the input grows?

335
00:19:58,420 --> 00:20:09,610

336
00:20:09,610 --> 00:20:10,860
We can even go further.

337
00:20:10,860 --> 00:20:13,130

338
00:20:13,130 --> 00:20:18,730
Do I actually care whether
it's 3,000 or 9,000?

339
00:20:18,730 --> 00:20:19,790
Well, I might.

340
00:20:19,790 --> 00:20:24,690
I might care whether a program
take say 3 hours to run or 9

341
00:20:24,690 --> 00:20:26,940
hours to run.

342
00:20:26,940 --> 00:20:31,730
But in fact, as it gets bigger,
and we really care

343
00:20:31,730 --> 00:20:34,120
about this as things get
bigger, I probably

344
00:20:34,120 --> 00:20:35,760
don't care that much.

345
00:20:35,760 --> 00:20:39,610
If I told you this was going to
take 3,000 years or 9,000

346
00:20:39,610 --> 00:20:42,030
years, you wouldn't care.

347
00:20:42,030 --> 00:20:44,520
Or probably, even if I told you
it was going to take 3,000

348
00:20:44,520 --> 00:20:48,650
days or 9,000 days, you'd say,
well, it's too long anyway.

349
00:20:48,650 --> 00:21:02,280
So typically, we even ignore
multiplicative constants and

350
00:21:02,280 --> 00:21:14,420
use a model of asymptotic growth
that talks about how

351
00:21:14,420 --> 00:21:19,340
the complexity grows as you
reach the limit of the sizes

352
00:21:19,340 --> 00:21:20,590
of the inputs.

353
00:21:20,590 --> 00:21:22,920

354
00:21:22,920 --> 00:21:31,000
This is typically done using
a notation we call big O

355
00:21:31,000 --> 00:21:38,244
notation written as a single
O. So if I write order n,

356
00:21:38,244 --> 00:21:44,940
O(n), what this says is this
algorithm, the complexity, the

357
00:21:44,940 --> 00:21:48,600
time grows linearly with n.

358
00:21:48,600 --> 00:21:51,180

359
00:21:51,180 --> 00:21:53,830
Doesn't say whether it's
3 times n or 2 times n.

360
00:21:53,830 --> 00:21:57,210
It's linear in n is
what this says.

361
00:21:57,210 --> 00:21:59,560
Well, why do we call it big O?

362
00:21:59,560 --> 00:22:02,130
Well, some people think it's
because, oh my God, this

363
00:22:02,130 --> 00:22:04,580
program will never end.

364
00:22:04,580 --> 00:22:06,720
But in fact, no.

365
00:22:06,720 --> 00:22:08,650
This notion was introduced
to computer

366
00:22:08,650 --> 00:22:11,500
science by Donald Knuth.

367
00:22:11,500 --> 00:22:15,620
And he chose the Greek letter
omicron because it was used in

368
00:22:15,620 --> 00:22:18,115
the 19th century by people
developing calculus.

369
00:22:18,115 --> 00:22:21,200

370
00:22:21,200 --> 00:22:23,420
We don't typically write
omicron because

371
00:22:23,420 --> 00:22:24,420
it's harder to types.

372
00:22:24,420 --> 00:22:29,480
So we usually use the capital
Latin letter O, hence, life

373
00:22:29,480 --> 00:22:31,260
gets simple.

374
00:22:31,260 --> 00:22:41,630
What this does is it gives
us an upper bound for the

375
00:22:41,630 --> 00:22:45,110
asymptotic growth
of the function.

376
00:22:45,110 --> 00:22:51,320
So formerly, we would write
something like f of x, where f

377
00:22:51,320 --> 00:22:57,810
is some function of
the input x, is

378
00:22:57,810 --> 00:23:03,460
order, let's say x squared.

379
00:23:03,460 --> 00:23:05,850
That would say it's quadratic
in the size of x.

380
00:23:05,850 --> 00:23:08,850

381
00:23:08,850 --> 00:23:14,470
Formally what this means is
that the function f--

382
00:23:14,470 --> 00:23:16,000
I should probably write
this down--

383
00:23:16,000 --> 00:23:22,120

384
00:23:22,120 --> 00:23:42,440
the function f grows no faster
than the quadratic

385
00:23:42,440 --> 00:23:52,040
polynomial x squared.

386
00:23:52,040 --> 00:23:57,070

387
00:23:57,070 --> 00:23:59,760
So let's look at what
this means.

388
00:23:59,760 --> 00:24:04,085
I wrote a little program that
talks about some of the--

389
00:24:04,085 --> 00:24:10,610
I should say probably most
popular values we see.

390
00:24:10,610 --> 00:24:16,680
So some of the most popular
orders we would write down, we

391
00:24:16,680 --> 00:24:20,100
often see order 1.

392
00:24:20,100 --> 00:24:23,550
And what that means
is constant.

393
00:24:23,550 --> 00:24:25,470
The time required is
independent of

394
00:24:25,470 --> 00:24:26,945
the size of the input.

395
00:24:26,945 --> 00:24:29,420
It doesn't say it's one step.

396
00:24:29,420 --> 00:24:30,880
But it's independent
of the input.

397
00:24:30,880 --> 00:24:33,240
It's constant.

398
00:24:33,240 --> 00:24:39,745
We often see order log n,
logarithmic growth.

399
00:24:39,745 --> 00:24:43,970

400
00:24:43,970 --> 00:24:45,990
Order n, linear.

401
00:24:45,990 --> 00:24:49,040

402
00:24:49,040 --> 00:24:52,140
One we'll see later this
week is nlog(n).

403
00:24:52,140 --> 00:24:55,130

404
00:24:55,130 --> 00:24:56,485
This is called log linear.

405
00:24:56,485 --> 00:24:59,520

406
00:24:59,520 --> 00:25:02,570
And we'll see why that occurs
surprisingly often.

407
00:25:02,570 --> 00:25:08,350

408
00:25:08,350 --> 00:25:14,050
Order n to the c where c is
some constant, this is

409
00:25:14,050 --> 00:25:15,300
polynomial.

410
00:25:15,300 --> 00:25:18,440

411
00:25:18,440 --> 00:25:23,530
A common polynomial would be
squared as in quadratic.

412
00:25:23,530 --> 00:25:27,930
And then, if we're terribly
unlucky, you run into things

413
00:25:27,930 --> 00:25:33,390
that are order c to the
n exponential in

414
00:25:33,390 --> 00:25:34,640
the size of the input.

415
00:25:34,640 --> 00:25:38,170

416
00:25:38,170 --> 00:25:42,220
To give you an idea of what
these classes actually mean, I

417
00:25:42,220 --> 00:25:45,740
wrote a little program that
produces some plots.

418
00:25:45,740 --> 00:25:49,050
Don't worry about what
the code looks like.

419
00:25:49,050 --> 00:25:50,830
In a few weeks, you'll
be able to write

420
00:25:50,830 --> 00:25:52,080
such programs yourself.

421
00:25:52,080 --> 00:25:55,600

422
00:25:55,600 --> 00:25:57,850
Not only will you be able
to, you'll be forced to.

423
00:25:57,850 --> 00:26:02,190

424
00:26:02,190 --> 00:26:05,510
So I'm just going to run this
and produce some plots showing

425
00:26:05,510 --> 00:26:06,760
different orders of growth.

426
00:26:06,760 --> 00:26:11,690

427
00:26:11,690 --> 00:26:11,854
All right.

428
00:26:11,854 --> 00:26:13,425
This is producing
these blocks.

429
00:26:13,425 --> 00:26:18,460

430
00:26:18,460 --> 00:26:19,500
Excuse me.

431
00:26:19,500 --> 00:26:20,750
I see.

432
00:26:20,750 --> 00:26:45,210

433
00:26:45,210 --> 00:26:47,950
So let's look at the plots.

434
00:26:47,950 --> 00:26:50,130
So here, I've plotted
linear growth

435
00:26:50,130 --> 00:26:51,670
versus logarithmic growth.

436
00:26:51,670 --> 00:26:54,680

437
00:26:54,680 --> 00:26:59,220
And as you can see, it's
quite a difference.

438
00:26:59,220 --> 00:27:03,060
If we can manage to get a
logarithmic algorithm, it

439
00:27:03,060 --> 00:27:06,490
grows much more slowly than
a linear algorithm.

440
00:27:06,490 --> 00:27:10,090
And we saw this when we looked
at the graded advantage of

441
00:27:10,090 --> 00:27:12,820
binary search as opposed
to linear search.

442
00:27:12,820 --> 00:27:17,230

443
00:27:17,230 --> 00:27:19,970
Actually, this is linear
versus log linear.

444
00:27:19,970 --> 00:27:21,460
What happened to figure one?

445
00:27:21,460 --> 00:27:24,040

446
00:27:24,040 --> 00:27:25,990
Well, we'll come back to it.

447
00:27:25,990 --> 00:27:31,360
So you'll see here that
log linear is

448
00:27:31,360 --> 00:27:33,910
much worse than linear.

449
00:27:33,910 --> 00:27:39,410
So this factor of nlog(n)
actually makes a considerable

450
00:27:39,410 --> 00:27:41,820
difference in running time.

451
00:27:41,820 --> 00:27:47,840

452
00:27:47,840 --> 00:27:52,310
Now, I'm going to compare a
log linear to quadratic, a

453
00:27:52,310 --> 00:27:54,480
small degree polynomial.

454
00:27:54,480 --> 00:27:58,230
As you can see, it almost looks
like log linear is not

455
00:27:58,230 --> 00:28:00,960
growing at all.

456
00:28:00,960 --> 00:28:05,410
So as bad as log linear looked
when we compared it to linear,

457
00:28:05,410 --> 00:28:10,860
we see that compared to
quadratic, it's pretty great.

458
00:28:10,860 --> 00:28:15,530
And what this tells us is that
in practice, even a quadratic

459
00:28:15,530 --> 00:28:19,880
algorithm is often impractically
slow, and we

460
00:28:19,880 --> 00:28:21,820
really can't use them.

461
00:28:21,820 --> 00:28:25,060
And so in practice, we worked
very hard to avoid even

462
00:28:25,060 --> 00:28:28,030
quadratic, which somehow
doesn't seem like it

463
00:28:28,030 --> 00:28:29,620
should be so bad.

464
00:28:29,620 --> 00:28:34,530
But in fact, as you can see,
it gets bad quickly.

465
00:28:34,530 --> 00:28:39,110
Yeah, this was the log versus
linear, not surprising.

466
00:28:39,110 --> 00:28:44,530
And now, if we look at quadratic
versus exponential,

467
00:28:44,530 --> 00:28:48,280
we can see hardly anything.

468
00:28:48,280 --> 00:28:53,520
And that's because exponential
is growing so quickly.

469
00:28:53,520 --> 00:29:00,270
So instead, what we're going to
do is I'm going to plot the

470
00:29:00,270 --> 00:29:06,620
y-axis logarithmically just so
we can actually see something.

471
00:29:06,620 --> 00:29:11,980
And as you can see on input of
size 1,000, an exponential

472
00:29:11,980 --> 00:29:15,933
algorithm is roughly order
10 to the 286th.

473
00:29:15,933 --> 00:29:18,630

474
00:29:18,630 --> 00:29:21,860
That's an unimaginably
large number.

475
00:29:21,860 --> 00:29:22,800
Right?

476
00:29:22,800 --> 00:29:25,070
I don't know what it compares
to, the number of atoms in the

477
00:29:25,070 --> 00:29:28,150
universe, or something
ridiculous, or maybe more.

478
00:29:28,150 --> 00:29:31,060
But we can't possibly think of
running an algorithm that's

479
00:29:31,060 --> 00:29:32,795
going to take this long.

480
00:29:32,795 --> 00:29:35,400
It's just not even
conceivable.

481
00:29:35,400 --> 00:29:39,150
So exponential, we sort of throw
up our hands and say

482
00:29:39,150 --> 00:29:40,590
we're dead.

483
00:29:40,590 --> 00:29:42,580
We can't do it.

484
00:29:42,580 --> 00:29:46,900
And so nobody uses exponential
algorithms for everything, yet

485
00:29:46,900 --> 00:29:48,090
for anything.

486
00:29:48,090 --> 00:29:52,850
Yet as we'll see, there are
problems that we care about

487
00:29:52,850 --> 00:29:56,160
that, in principle, can only
be solved by exponential

488
00:29:56,160 --> 00:29:57,410
algorithms.

489
00:29:57,410 --> 00:29:59,390

490
00:29:59,390 --> 00:30:00,410
So what do we do?

491
00:30:00,410 --> 00:30:02,330
As we'll see, well, we
usually don't try

492
00:30:02,330 --> 00:30:03,610
and solve those problems.

493
00:30:03,610 --> 00:30:04,530
We try and solve some

494
00:30:04,530 --> 00:30:08,310
approximation to those problems.

495
00:30:08,310 --> 00:30:13,000
Or we use some other tricks to
say, well, we know the worst

496
00:30:13,000 --> 00:30:14,800
case will be terrible, but
here's how we're going to

497
00:30:14,800 --> 00:30:16,790
avoid the worst case.

498
00:30:16,790 --> 00:30:20,130
We'll see a lot of that towards
the end of the term.

499
00:30:20,130 --> 00:30:25,140
The moral is try not to do
anything that's worse than log

500
00:30:25,140 --> 00:30:26,690
linear if you possibly can.

501
00:30:26,690 --> 00:30:30,110

502
00:30:30,110 --> 00:30:36,130
Now some truth in advertising,
some caveats.

503
00:30:36,130 --> 00:30:41,420
If I look at my definition of
what big O means, I said it

504
00:30:41,420 --> 00:30:43,520
grows no faster than.

505
00:30:43,520 --> 00:30:46,730
So in principle, I could say,
well, what the heck, I'll just

506
00:30:46,730 --> 00:30:49,360
write 2 to the x here.

507
00:30:49,360 --> 00:30:50,340
And it's still true.

508
00:30:50,340 --> 00:30:51,590
It's not faster than that.

509
00:30:51,590 --> 00:30:54,520

510
00:30:54,520 --> 00:30:56,850
It's not what we actually
want to do.

511
00:30:56,850 --> 00:31:00,680
What we actually want
is a type bound.

512
00:31:00,680 --> 00:31:03,650
We'd like to say it's no faster
than this, but it's no

513
00:31:03,650 --> 00:31:08,260
slower than this either, to try
and characterize the worst

514
00:31:08,260 --> 00:31:11,360
cases precisely as we can.

515
00:31:11,360 --> 00:31:15,860
Formally speaking, a theorist
used something called big

516
00:31:15,860 --> 00:31:17,930
Theta notation for this.

517
00:31:17,930 --> 00:31:22,930
They write a theta instead of
an O. However, most of the

518
00:31:22,930 --> 00:31:31,010
time in practice, when somebody
writes something like

519
00:31:31,010 --> 00:31:39,580
f of x is order x squared, what
they mean is the worst

520
00:31:39,580 --> 00:31:44,140
case is really about
x squared.

521
00:31:44,140 --> 00:31:45,900
And that's the way we're
going to use it here.

522
00:31:45,900 --> 00:31:48,030
We're not going to try
and get too formal.

523
00:31:48,030 --> 00:31:51,150
We're going to do what people
actually do in practice when

524
00:31:51,150 --> 00:31:55,940
they talk about complexity.

525
00:31:55,940 --> 00:31:57,590
All right, let's look at
another example now.

526
00:31:57,590 --> 00:32:03,770

527
00:32:03,770 --> 00:32:07,870
Here, I've written factorial
recursively.

528
00:32:07,870 --> 00:32:11,360
Didn't even try to disguise
what it was.

529
00:32:11,360 --> 00:32:14,530
So let's think about how
we would analyze the

530
00:32:14,530 --> 00:32:17,930
complexity of this.

531
00:32:17,930 --> 00:32:21,650
Well, we know that we can ignore
the first two lines of

532
00:32:21,650 --> 00:32:24,420
code because those are just
the additives pieces.

533
00:32:24,420 --> 00:32:27,090
We don't care about that--

534
00:32:27,090 --> 00:32:33,910
the first line, and the
if, and the return.

535
00:32:33,910 --> 00:32:37,700
So what's the piece
we care about?

536
00:32:37,700 --> 00:32:42,820
We care about the number of
times the factorial is called.

537
00:32:42,820 --> 00:32:46,480
In the first implementation of
factorial, we cared about the

538
00:32:46,480 --> 00:32:49,580
number of iterations
of a loop.

539
00:32:49,580 --> 00:32:53,560
Now instead of using a loop, you
use recursion to do more

540
00:32:53,560 --> 00:32:55,830
or less the same thing.

541
00:32:55,830 --> 00:32:58,050
And so we care about
the number of

542
00:32:58,050 --> 00:33:00,165
times fact is called.

543
00:33:00,165 --> 00:33:04,800

544
00:33:04,800 --> 00:33:06,410
How many times will
that happen?

545
00:33:06,410 --> 00:33:13,710

546
00:33:13,710 --> 00:33:17,660
Well, let's think about why I
know this doesn't run forever,

547
00:33:17,660 --> 00:33:19,850
because that's always the way
we really think about

548
00:33:19,850 --> 00:33:22,420
complexity in some sense.

549
00:33:22,420 --> 00:33:25,260
I know it doesn't run forever
because each time I call

550
00:33:25,260 --> 00:33:29,410
factorial, I call it on a number
one smaller than the

551
00:33:29,410 --> 00:33:30,660
number before.

552
00:33:30,660 --> 00:33:33,160

553
00:33:33,160 --> 00:33:36,650
So how many times can
I do that if I start

554
00:33:36,650 --> 00:33:39,500
with a number n?

555
00:33:39,500 --> 00:33:41,680
n times, right?

556
00:33:41,680 --> 00:33:45,910
So once again, it's order n.

557
00:33:45,910 --> 00:33:50,090
So the interesting thing we see
here is that essentially,

558
00:33:50,090 --> 00:33:52,490
I've given you the same
algorithm recursively and

559
00:33:52,490 --> 00:33:54,080
iteratively.

560
00:33:54,080 --> 00:33:57,840
Not surprisingly, even though
I've coded it differently,

561
00:33:57,840 --> 00:34:00,890
it's the same complexity.

562
00:34:00,890 --> 00:34:04,700
Now in practice, the recursive
one might take a little longer

563
00:34:04,700 --> 00:34:08,330
to run, because there's a
certain overhead to function

564
00:34:08,330 --> 00:34:12,010
calls that we don't have
with while loops.

565
00:34:12,010 --> 00:34:15,130
But we don't actually
care about that.

566
00:34:15,130 --> 00:34:19,020
Its overhead is one of those
multiplicative constants I

567
00:34:19,020 --> 00:34:21,110
said we're going to ignore.

568
00:34:21,110 --> 00:34:24,580
And in fact, it's a very small
multiplicative constant.

569
00:34:24,580 --> 00:34:27,500
It really doesn't make
much of a difference.

570
00:34:27,500 --> 00:34:31,310
So how do I decide whether to
use recursion or iteration has

571
00:34:31,310 --> 00:34:34,400
nothing to do with efficiency,
it's whichever is more

572
00:34:34,400 --> 00:34:36,550
convenient to code.

573
00:34:36,550 --> 00:34:39,610
In this case, I kind of like
the fact that recursive

574
00:34:39,610 --> 00:34:42,260
factorial is a little neater.

575
00:34:42,260 --> 00:34:45,969
So that's what I would use
and not worry about the

576
00:34:45,969 --> 00:34:47,219
efficiency.

577
00:34:47,219 --> 00:34:50,020

578
00:34:50,020 --> 00:34:50,200
All right.

579
00:34:50,200 --> 00:34:51,570
Let's look at another example.

580
00:34:51,570 --> 00:34:56,090

581
00:34:56,090 --> 00:34:57,340
How about g?

582
00:34:57,340 --> 00:35:01,300

583
00:35:01,300 --> 00:35:03,375
What's the complexity of g?

584
00:35:03,375 --> 00:35:07,200
Well, I can ignore the
first statement.

585
00:35:07,200 --> 00:35:09,470
But now I've got two
nested loops.

586
00:35:09,470 --> 00:35:12,410
How do I go and think
about this?

587
00:35:12,410 --> 00:35:17,770
The way I do it is I start by
finding the inner loop.

588
00:35:17,770 --> 00:35:21,210

589
00:35:21,210 --> 00:35:23,480
How many times do I go through
the inner loop?

590
00:35:23,480 --> 00:35:29,750

591
00:35:29,750 --> 00:35:34,660
I go through the inner
loop n times, right?

592
00:35:34,660 --> 00:35:36,990
So it executes the inner
for statement is

593
00:35:36,990 --> 00:35:40,330
going to be order n.

594
00:35:40,330 --> 00:35:45,130
The next question I ask is how
many times do I start the

595
00:35:45,130 --> 00:35:46,380
inner loop up again?

596
00:35:46,380 --> 00:35:48,740

597
00:35:48,740 --> 00:35:51,910
That's also order n times.

598
00:35:51,910 --> 00:35:53,640
So what's the complexity
of this?

599
00:35:53,640 --> 00:36:00,240

600
00:36:00,240 --> 00:36:02,408
Somebody?

601
00:36:02,408 --> 00:36:03,806
AUDIENCE: n-squared.

602
00:36:03,806 --> 00:36:04,740
PROFESSOR: Yes.

603
00:36:04,740 --> 00:36:06,970
I think I heard the
right answer.

604
00:36:06,970 --> 00:36:10,155
It's order n-squared.

605
00:36:10,155 --> 00:36:13,730

606
00:36:13,730 --> 00:36:18,050
Because I execute the inner
loop n times, or each time

607
00:36:18,050 --> 00:36:22,340
around is n, then I multiply it
by n because I'm doing the

608
00:36:22,340 --> 00:36:23,900
outer loop n times.

609
00:36:23,900 --> 00:36:25,620
So the inner loop is
order n-squared.

610
00:36:25,620 --> 00:36:30,060

611
00:36:30,060 --> 00:36:31,350
That makes sense?

612
00:36:31,350 --> 00:36:34,820
So typically, whenever I have
nested loops, I have to do

613
00:36:34,820 --> 00:36:37,490
this kind of reasoning.

614
00:36:37,490 --> 00:36:41,080
Same thing if I have recursion
inside a loop or nested

615
00:36:41,080 --> 00:36:42,970
recursions.

616
00:36:42,970 --> 00:36:47,120
I start at the inside and
work my way out is the

617
00:36:47,120 --> 00:36:48,370
way I do the analysis.

618
00:36:48,370 --> 00:36:51,820

619
00:36:51,820 --> 00:36:53,290
Let's look at another example.

620
00:36:53,290 --> 00:36:55,840
It's kind of a different take.

621
00:36:55,840 --> 00:36:57,090
How about h?

622
00:36:57,090 --> 00:36:59,980

623
00:36:59,980 --> 00:37:01,580
What's the complexity of h?

624
00:37:01,580 --> 00:37:08,230

625
00:37:08,230 --> 00:37:09,560
First of all, what's h doing?

626
00:37:09,560 --> 00:37:12,920

627
00:37:12,920 --> 00:37:14,290
Kind of always a good
way to start.

628
00:37:14,290 --> 00:37:20,000

629
00:37:20,000 --> 00:37:21,540
What is answer going to be?

630
00:37:21,540 --> 00:37:33,160

631
00:37:33,160 --> 00:37:34,764
Yeah?

632
00:37:34,764 --> 00:37:38,103
AUDIENCE: The sum of the
[UNINTELLIGIBLE].

633
00:37:38,103 --> 00:37:39,165
PROFESSOR: Right.

634
00:37:39,165 --> 00:37:39,790
Exactly.

635
00:37:39,790 --> 00:37:41,560
It's going to be the
sum of the digits.

636
00:37:41,560 --> 00:37:47,570

637
00:37:47,570 --> 00:37:49,640
Spring training is already
under way,

638
00:37:49,640 --> 00:37:53,160
so sum of the digits.

639
00:37:53,160 --> 00:37:55,080
And what's the complexity?

640
00:37:55,080 --> 00:37:58,530
Well, we can analyze it.

641
00:37:58,530 --> 00:38:00,000
Right away, we know
we can ignore

642
00:38:00,000 --> 00:38:01,970
everything except the loop.

643
00:38:01,970 --> 00:38:04,060
So how many times do I
go through this loop?

644
00:38:04,060 --> 00:38:07,030

645
00:38:07,030 --> 00:38:10,910
It depends upon the number
of digits in the string

646
00:38:10,910 --> 00:38:15,940
representation of
the int, right?

647
00:38:15,940 --> 00:38:22,810
Now, if I were careless, I would
write something like

648
00:38:22,810 --> 00:38:36,110
order n, where n is the
number of digits in s.

649
00:38:36,110 --> 00:38:39,060
But really, I'm not allowed
to do that.

650
00:38:39,060 --> 00:38:41,480
Why not?

651
00:38:41,480 --> 00:38:45,190
Because I have to express the
complexity in terms of the

652
00:38:45,190 --> 00:38:47,610
inputs to the program.

653
00:38:47,610 --> 00:38:50,290
And s is not an input.

654
00:38:50,290 --> 00:38:53,190
s is a local variable.

655
00:38:53,190 --> 00:38:56,870
So somehow, I'm going to have to
express the complexity, not

656
00:38:56,870 --> 00:39:03,620
in terms of s, but
in terms of what?

657
00:39:03,620 --> 00:39:04,870
x.

658
00:39:04,870 --> 00:39:08,450

659
00:39:08,450 --> 00:39:11,030
So that's no go.

660
00:39:11,030 --> 00:39:12,500
So what is in terms of x?

661
00:39:12,500 --> 00:39:16,160

662
00:39:16,160 --> 00:39:17,150
How many digits?

663
00:39:17,150 --> 00:39:17,480
Yeah?

664
00:39:17,480 --> 00:39:18,794
AUDIENCE: Is it constant?

665
00:39:18,794 --> 00:39:20,110
PROFESSOR: It's not constant.

666
00:39:20,110 --> 00:39:20,830
No.

667
00:39:20,830 --> 00:39:22,660
Because I'll have more
digits in a billion

668
00:39:22,660 --> 00:39:23,910
than I will in four.

669
00:39:23,910 --> 00:39:29,220

670
00:39:29,220 --> 00:39:29,630
Right.

671
00:39:29,630 --> 00:39:30,510
Log --

672
00:39:30,510 --> 00:39:33,456
in this case, base 10 of x.

673
00:39:33,456 --> 00:39:37,110

674
00:39:37,110 --> 00:39:42,395
The number of decimal digits
required to express an integer

675
00:39:42,395 --> 00:39:44,620
is the log of the magnitude
of that integer.

676
00:39:44,620 --> 00:39:48,370

677
00:39:48,370 --> 00:39:51,630
You think about we looked at
binary numbers and decimal

678
00:39:51,630 --> 00:39:57,700
numbers last lecture, that was
exactly what we were doing.

679
00:39:57,700 --> 00:40:00,780
So that's the way I have
to express this.

680
00:40:00,780 --> 00:40:04,080
Now, what's the moral here?

681
00:40:04,080 --> 00:40:07,040
The thing I really care about
is not that this is how you

682
00:40:07,040 --> 00:40:10,110
talk about the number
of digits in an int.

683
00:40:10,110 --> 00:40:12,700
What I care about is
that you always

684
00:40:12,700 --> 00:40:14,360
have to be very careful.

685
00:40:14,360 --> 00:40:17,720
People often think that they're
done when they write

686
00:40:17,720 --> 00:40:20,350
something like order n.

687
00:40:20,350 --> 00:40:25,010
But they're not until they tell
you what n means, Because

688
00:40:25,010 --> 00:40:26,260
that can be pretty subtle.

689
00:40:26,260 --> 00:40:29,770

690
00:40:29,770 --> 00:40:37,490
Order x would have been wrong
because it's not the magnitude

691
00:40:37,490 --> 00:40:39,230
of the integer x.

692
00:40:39,230 --> 00:40:44,140
It's this is what controls
the growth.

693
00:40:44,140 --> 00:40:47,190
So whenever you're looking at
complexity, you have to be

694
00:40:47,190 --> 00:40:51,840
very careful what you mean,
what the variables are.

695
00:40:51,840 --> 00:40:54,330
This is particularly true now
when you look at functions say

696
00:40:54,330 --> 00:40:55,815
with multiple inputs.

697
00:40:55,815 --> 00:41:03,280

698
00:41:03,280 --> 00:41:03,405
Ok.

699
00:41:03,405 --> 00:41:06,660
Let's look at some
more examples.

700
00:41:06,660 --> 00:41:09,940
So we've looked before
at search.

701
00:41:09,940 --> 00:41:13,840
So this is code you've
seen before, really.

702
00:41:13,840 --> 00:41:17,330
Here's a linear search
and a binary search.

703
00:41:17,330 --> 00:41:21,750
And in fact, informally, we've
looked at the complexity of

704
00:41:21,750 --> 00:41:23,000
these things before.

705
00:41:23,000 --> 00:41:28,020

706
00:41:28,020 --> 00:41:31,350
And we can run them, and we can
see how they will grow.

707
00:41:31,350 --> 00:41:34,240
But it won't surprise you.

708
00:41:34,240 --> 00:41:35,830
So if we look at the
linear search--

709
00:41:35,830 --> 00:41:44,650

710
00:41:44,650 --> 00:41:49,270
whoops, I'm printing
the values here,

711
00:41:49,270 --> 00:41:50,520
just shows it works.

712
00:41:50,520 --> 00:41:54,200

713
00:41:54,200 --> 00:41:58,450
But now, the binary search, what
we're going to look at is

714
00:41:58,450 --> 00:41:59,340
how it grows.

715
00:41:59,340 --> 00:42:01,490
This is exactly the search
we looked at before.

716
00:42:01,490 --> 00:42:25,440

717
00:42:25,440 --> 00:42:28,640
And the thing I want you to
notice, and as we looked at

718
00:42:28,640 --> 00:42:33,330
before, we saw it was
logarithmic is that as the

719
00:42:33,330 --> 00:42:38,060
size of the list grows, doubles,
I only need one more

720
00:42:38,060 --> 00:42:40,370
step to do the search.

721
00:42:40,370 --> 00:42:43,780
This is the beauty of a
logarithmic algorithm.

722
00:42:43,780 --> 00:42:46,770
So as I go from a 100, which
takes 7 steps, to 200,

723
00:42:46,770 --> 00:42:49,065
it only takes 8.

724
00:42:49,065 --> 00:42:51,000
1,600 takes 11.

725
00:42:51,000 --> 00:42:53,570
And when I'm all the way up to
some very big number, and I'm

726
00:42:53,570 --> 00:42:57,880
not even sure what that number
is, it took 23 steps.

727
00:42:57,880 --> 00:42:59,500
But very slow growth.

728
00:42:59,500 --> 00:43:03,510

729
00:43:03,510 --> 00:43:05,170
So that's a good thing.

730
00:43:05,170 --> 00:43:08,190

731
00:43:08,190 --> 00:43:10,820
What's the order of this?

732
00:43:10,820 --> 00:43:13,550
It's order n where n is what?

733
00:43:13,550 --> 00:43:16,130
Order log n where n is what?

734
00:43:16,130 --> 00:43:20,530

735
00:43:20,530 --> 00:43:22,090
Let's just try and write
it carefully.

736
00:43:22,090 --> 00:43:27,660

737
00:43:27,660 --> 00:43:31,275
Well, it's order length
of the list.

738
00:43:31,275 --> 00:43:37,770

739
00:43:37,770 --> 00:43:42,740
We don't care what the actual
members of the list are.

740
00:43:42,740 --> 00:43:47,410
Now, that's an interesting
question to ask.

741
00:43:47,410 --> 00:43:49,000
Let's look at the code
for a minute.

742
00:43:49,000 --> 00:43:51,830
Is that a valid assumption?

743
00:43:51,830 --> 00:43:54,670
Well, it seems to be when
we look at my test.

744
00:43:54,670 --> 00:43:57,260
But let's look at what
I'm doing here.

745
00:43:57,260 --> 00:43:59,860
So a couple of things
I want to point out.

746
00:43:59,860 --> 00:44:05,700
One is I used a very common
trick when dealing with these

747
00:44:05,700 --> 00:44:07,920
kinds of algorithms.

748
00:44:07,920 --> 00:44:11,180
You'll notice that I have
something called bsearch and

749
00:44:11,180 --> 00:44:12,430
something called search.

750
00:44:12,430 --> 00:44:15,170

751
00:44:15,170 --> 00:44:18,550
All search does is
called bsearch.

752
00:44:18,550 --> 00:44:20,730
Why did I even bother
with search?

753
00:44:20,730 --> 00:44:24,430
Why didn't I just with my code
down here call bsearch with

754
00:44:24,430 --> 00:44:25,680
some initial values?

755
00:44:25,680 --> 00:44:28,520

756
00:44:28,520 --> 00:44:35,615
The answer is really, I started
with this search.

757
00:44:35,615 --> 00:44:38,340

758
00:44:38,340 --> 00:44:45,270
And a user of search shouldn't
have to worry that I got

759
00:44:45,270 --> 00:44:48,990
clever and went from this linear
search to a binary

760
00:44:48,990 --> 00:44:53,030
search or maybe some more
complex search yet.

761
00:44:53,030 --> 00:44:56,730
I need to have a consistent
interface

762
00:44:56,730 --> 00:44:59,660
for the search function.

763
00:44:59,660 --> 00:45:03,260
And the interface is what it
looks like to the caller.

764
00:45:03,260 --> 00:45:05,210
And it says when I call
it, it just takes

765
00:45:05,210 --> 00:45:08,070
a list and an element.

766
00:45:08,070 --> 00:45:12,690
It shouldn't have to take the
high bound and the lower bound

767
00:45:12,690 --> 00:45:13,590
as arguments.

768
00:45:13,590 --> 00:45:15,400
Because that really is
not intrinsic to

769
00:45:15,400 --> 00:45:18,690
the meaning of search.

770
00:45:18,690 --> 00:45:24,100
So I typically will organize
my program by having this

771
00:45:24,100 --> 00:45:28,180
search look exactly like this
search to a caller.

772
00:45:28,180 --> 00:45:30,250
And then, it does whatever
it needs to do

773
00:45:30,250 --> 00:45:31,890
to call binary search.

774
00:45:31,890 --> 00:45:36,190

775
00:45:36,190 --> 00:45:39,110
So that's usually the way
you do these things.

776
00:45:39,110 --> 00:45:42,960
It's very common with recursive
algorithms, various

777
00:45:42,960 --> 00:45:45,540
things where you need some
initial value that's only

778
00:45:45,540 --> 00:45:48,360
there for the initial call,
things like that.

779
00:45:48,360 --> 00:45:53,230

780
00:45:53,230 --> 00:45:54,160
Let me finish--

781
00:45:54,160 --> 00:46:00,580
wanted to point out is the use
of this global variable.

782
00:46:00,580 --> 00:46:04,680
So you'll notice down
here, I define

783
00:46:04,680 --> 00:46:05,930
something called NumCalls.

784
00:46:05,930 --> 00:46:09,260

785
00:46:09,260 --> 00:46:11,970
Remember we talked
about scopes.

786
00:46:11,970 --> 00:46:16,520
So this is now an identifier
that exists in the outermost

787
00:46:16,520 --> 00:46:17,770
scope of the program.

788
00:46:17,770 --> 00:46:20,090

789
00:46:20,090 --> 00:46:22,590
Then in bsearch, I used it.

790
00:46:22,590 --> 00:46:25,410

791
00:46:25,410 --> 00:46:29,850
But I said I'm going to use
this global variable, this

792
00:46:29,850 --> 00:46:33,110
variable declared outside
the scope of

793
00:46:33,110 --> 00:46:36,560
bsearch inside bsearch.

794
00:46:36,560 --> 00:46:39,920
So it's this statement that
tells me not to create a new

795
00:46:39,920 --> 00:46:45,880
local variable here but to use
the one in the outer scope.

796
00:46:45,880 --> 00:46:51,460
This is normally considered
poor programming practice.

797
00:46:51,460 --> 00:46:53,450
Global variables can
often lead to

798
00:46:53,450 --> 00:46:56,190
very confusing programs.

799
00:46:56,190 --> 00:46:58,590
Occasionally, they're useful.

800
00:46:58,590 --> 00:47:00,580
Here it's pretty useful because
I'm just trying to

801
00:47:00,580 --> 00:47:04,650
keep track of a number of times
this thing is called.

802
00:47:04,650 --> 00:47:08,830
And so I don't want a new
variable generated each time

803
00:47:08,830 --> 00:47:10,080
it's instantiated.

804
00:47:10,080 --> 00:47:14,460

805
00:47:14,460 --> 00:47:16,610
Now, you had a question?

806
00:47:16,610 --> 00:47:17,613
Yeah?

807
00:47:17,613 --> 00:47:19,070
AUDIENCE: Just checking.

808
00:47:19,070 --> 00:47:23,700
The order len L, the size
of the list is the

809
00:47:23,700 --> 00:47:25,670
order of which search?

810
00:47:25,670 --> 00:47:28,550
PROFESSOR: That's the order
of the linear search.

811
00:47:28,550 --> 00:47:36,050
The order of the binary search
is order log base 2 of L--

812
00:47:36,050 --> 00:47:38,890

813
00:47:38,890 --> 00:47:40,850
sorry, not of L, right?

814
00:47:40,850 --> 00:47:46,920
Doesn't make sense to take
the log of a list of the

815
00:47:46,920 --> 00:47:48,170
length of the list.

816
00:47:48,170 --> 00:47:53,100

817
00:47:53,100 --> 00:47:57,460
Typically, we don't bother
writing base 2.

818
00:47:57,460 --> 00:47:59,670
If it's logarithmic, it doesn't
really very much

819
00:47:59,670 --> 00:48:01,280
matter what the base is.

820
00:48:01,280 --> 00:48:04,130
You'll still get that
very slow growth.

821
00:48:04,130 --> 00:48:08,420
Log base 10, log base 2, not
that much difference.

822
00:48:08,420 --> 00:48:12,560
So we typically just
write log.

823
00:48:12,560 --> 00:48:12,990
All right.

824
00:48:12,990 --> 00:48:14,910
People with me?

825
00:48:14,910 --> 00:48:25,920
Now, for this to be true, or in
fact, even if we go look at

826
00:48:25,920 --> 00:48:32,726
the linear search, there's
kind of an assumption.

827
00:48:32,726 --> 00:48:35,990

828
00:48:35,990 --> 00:48:40,650
I'm assuming that I can extract
the elements from a

829
00:48:40,650 --> 00:48:49,340
list and compare them to a
value in constant time.

830
00:48:49,340 --> 00:48:52,180
Because remember, my model of
computation says that every

831
00:48:52,180 --> 00:48:57,120
step takes the same amount
of time, roughly.

832
00:48:57,120 --> 00:49:02,140
And if I now look say a binary
search, you'll see I'm doing

833
00:49:02,140 --> 00:49:04,270
something that apparently
looks a little bit

834
00:49:04,270 --> 00:49:06,630
complicated up here.

835
00:49:06,630 --> 00:49:10,700

836
00:49:10,700 --> 00:49:14,010
I am looking at L of low and
comparing it to e, and L of

837
00:49:14,010 --> 00:49:15,710
high and comparing it to e.

838
00:49:15,710 --> 00:49:18,920

839
00:49:18,920 --> 00:49:22,590
How do I know that's
constant time?

840
00:49:22,590 --> 00:49:28,130
Maybe it takes me order length
of list time to extract the

841
00:49:28,130 --> 00:49:30,370
last element.

842
00:49:30,370 --> 00:49:34,720
So I've got to be very careful
when I look at complexity, not

843
00:49:34,720 --> 00:49:38,830
to think I only have to look
at the complexity of the

844
00:49:38,830 --> 00:49:41,720
program itself, that is to say,
in this case, the number

845
00:49:41,720 --> 00:49:45,510
of recursive calls, but is
there something that it's

846
00:49:45,510 --> 00:49:49,680
doing inside this function that
might be more complex

847
00:49:49,680 --> 00:49:50,930
than I think.

848
00:49:50,930 --> 00:49:52,950

849
00:49:52,950 --> 00:49:57,930
As it happens, in this case,
this rather complicated

850
00:49:57,930 --> 00:50:01,900
expression can be done
in constant time.

851
00:50:01,900 --> 00:50:03,370
And that will be the
first topic of

852
00:50:03,370 --> 00:50:04,620
the lecture on Thursday.

853
00:50:04,620 --> 00:50:09,620